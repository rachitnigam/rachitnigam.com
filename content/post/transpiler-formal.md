+++
title = 'Formally speaking, "Transpiler" is a useless word'
date = 2025-12-30
draft = false

[extra]
summary = "Tools of math to wage a pointless war"
+++

[I find "transpiler" to be a useless word](../transpiler): depending on the speaker, it can mean anything from a text replacement tool to a sophisticated optimizing compiler; I would much rather just use "compiler" to describe these systems. In a display of insight, a commenter pointed me to the dichotomy between lumpers and splitters: people who try to group similar things vs. people who try to separate out different things. I think this is an interesting idea and made me want to pen down, rigorously and formally, *why* I think compilers and transpilers are the same thing.

So, here is the game plan: I will introduce a couple of ideas that the programming languages (PL) community developed to carefully think about tools that manipulate programs. I will use these ideas to give three formal definitions of the word "transpiler" and work through why I do not like them. Even if you disagree, I hope that the formal framework provides a more rigorous ground for debate!

# It Walks Like a Duck

*Concrete syntax* is the first layer of a programming language we interact with: the symbols and words that form a language's vocabulary.  The PL community formalizes the syntax of a language using the Backus-Naur form (BNF) grammar. For example, here is the syntax of simple imperative language with loops and conditionals:

$$
\begin{aligned}
n &\in \mathcal{N} \quad b \in \\{T, F\\} \quad x \in variables \\\\
e &::= n | e + e | e - e | ... \\\\
c &::= b | e > e | e = e | c\\, \texttt{and}\\, c | ... \\\\
s &::= x := e \\\\
  &|\\, s;s \\\\
  &|\\, \texttt{if}(c)\\,\\{s\\}\\,\texttt{else}\\,\\{s\\} \\\\
  &|\\, \texttt{while}\\, c\\, \\{ s \\} | ...
\end{aligned}
$$
A BNF grammar is _recursively-defined_: the generator for expressions (\\(e\\)) defines terminals, such as numbers (\\(n\\)), as well as compound expressions, such as \\( e - e \\), which refer back to the generator. Other generators, such as statements (\\(s\\)) mention expressions as well as special tokens such as `if` and `while`. Formally, a programming language is the (infinite) set of all terms generated by this recursive definition:

> **Syntax of a Programming Language:** A programming language (\\(L\\)) is the set of all terms enumerated by some BNF grammar (\\(G\\)).
> Therefore \\(L = \\{ t | t \in G \\} \\).

Here's an intuitive way to think about this definition: there is a universe of *all possible terms* that can be constructed by combining the symbols in ASCII. Let's call this *the alphabet soup*. A BNF grammar provides structure to the alphabet soup by saying which combinational of terms are a part of some language and which ones are not. For example, the alphabet soup contains the term `>+++` the but language above does not. However, you can totally define it to be a meaningful term in another language.

Using this idea of syntax, I give my first definition of the word "transpiler":

> **Transpilers are Syntax Translators.** A *Transpiler* transforms programs in a source language (S) into programs a target language (T) by converting syntax constructs in S to match the "most natural" ones in T.

"Most natural" is my attempt to capture what people mean when they say "transpilers compile two languages at the same level of abstraction". For example, when transpiling JavaScript programs to Python, `for (...) { body }`  is converted into `for ...: body`. Unfortunately, when the target language does not have the same constructs as the source language, the "transpiler" needs to be able to do something else: it needs to *preserve the intention* of the programmer when translating between S and T. The formalism of syntax does not give us the ability to talk about the intention of the programmer. Let's look at the next formalism.

# It Quacks Like a Duck

The *execution model* is the second layer of our interaction with programming languages: it defines how a concrete program is transformed into computation. PL researchers call this the *semantics* of the language and it is sacrosanct: by precisely describing what a program means, a semantics also defines what optimizations are valid, what guarantees a type system gives, and what intention of the programmer the compiler must preserve when translating a program between languages.

There are many different flavors of formal semantics but I will talk about *operational semantics*. An operational semantics describes how to evaluate programs by defining a mathematical "machine" that rewrites programs into simpler ones (or *reduces* them). For example, let's consider the expression: `(1 + 2) > (3 + 4)`. Intuitively, we expect the following steps to occurs when this program runs:

```python
(1 + 2) > (3 + 4)
3 > (3 + 4)
3 > 7
False
```

Each one of these steps is a "reduction" within an operational semantics. We can formalize the possibly reductions using a *big-step* operational semantics which defines the relation \\(e_1 \Downarrow e_2\\) (read as "\\(e_1\\) reduces to \\(e_2\\)"):

$$
\begin{aligned}
&\frac{e_1 \Downarrow v_1 \quad e_2 \Downarrow v_2 \quad v = v_1 + v_2}{e_1 + e_2 \Downarrow v}
\\\\
&\frac{c_1 \Downarrow v_1 \quad c_2 \Downarrow v_2 \quad v_1 > v_2}{c_1 > c_2 \Downarrow \texttt{True}}
\\\\
&\frac{c_1 \Downarrow v_1 \quad c_2 \Downarrow v_2 \quad v_1 < v_2}{c_1 < c_2 \Downarrow \texttt{False}}
\end{aligned}
$$

Like the concrete syntax definition, the rules are recursive: in order to reduce large expressions, we first reduce their subcomponents and then perform some computation over the simplified values.

> **Tricks for encoding operational semantics.** There are two interesting tricks we play when defining the operators \\(+\\) and \\(>\\) in the rules above.
> 1. The first rule reduces two expressions into values and then applies the \\(+\\) symbol. But this seems circular: to define \\(+\\), we use \\(+\\). Absolute rigor would demand that we further describe the \\(+\\) operator on numbers \\(v_1\\) and \\(v_2\\) using some sort of encoding such as Peano arithmetic or Church numerals.
> 2. The \\(>\\) operator has two rules: one for the `True` case and another for the `False` case. This is another common trick in defining operational semantics: we can encode choice by describing multiple rules. Because the two rules share almost all the conditions on the top, they will attempt to reduce expressions like \\(c_1 > c_2\\) but only one of them will succeed for any given expression. For the acutely philosophical, we are using the built-in notion of "matching" within inference rules to define a conditional; this means that inference rules themselves have a semantics...

Let's see why semantics are useful by studying a particular compilation example. First, we define our fragments of our source (\\(S\\)) and target (\\(T\\)) languages:

$$
\begin{aligned}
x &\in variables \\\\
e_s &::= b\\, |\\, e > e\\, |\\, e\\, \texttt{and}\\, e \\\\
e_t &::= b\\, |\\, x\\, \texttt{and}\\, x \\\\
	&|\\, \texttt{let}\\, x = e\\, \texttt{in}\\, e \\\\
	&|\\, \texttt{if}\\, x\\, \texttt{then}\\, e\\, \texttt{else}\\, e
\end{aligned}
$$
Let's walk through the differences in the two languages by comparing the syntax for \\(\texttt{and}\\):
the expression \\((1 > 2) \\,\texttt{and}\\, (3 > 4)\\) is a valid expression
in the source language (\\(e_s\\)) but not the target language (\\(e_t\\)). This is because the syntax for
\\(\texttt{and}\\) for the target language requires the sub-expressions to be variables (\\(x\\)).

Given these definitions, let's try to build a "transpiler" using our first definition. Staying true to our definition of translating to the most natural operator within \\(e_t\\), we would like to generate another \\(\texttt{and}\\) expression. To do this, we need to introduce some `let`-bindings to name our expressions:

```ocaml
e1 && e2
(* Rewrite into *)
let x1 = e1 in
let x2 = e2 in
x1 && x2
```

Transpilation complete! Except, this rewritten program has introduced errors that were not possible before. In Python, we can write:

```python
if x is not None and x > 10: ...
```

Our rewrite rules transforms this into:

```python
p = x is not None
q = x > 10
if p and q: ...
```

Executing this will generate and error saying that comparisons are not supported between `NoneType` and `int`. However, this error cannot occur in the first program. This is because `and` implements *short-circuiting* behavior: if the left side evaluates to `False`, the right side is not evaluated at all. By binding both the expressions *before* the `and` operation, we've eliminated the short-circuiting. This highlights the folly of thinking about "transpilation" as a purely syntactic process; if you ignore the semantics, you change the meaning of programs.

An operational semantics can differentiate between the two possible semantics of the \\(\texttt{and}\\) operator. This first rule, which looks very similar to the operational semantics above, captures a *non-short-circuiting* implementation of \\(\texttt{and}\\):
$$
\begin{aligned}
&\frac{e_1 \Downarrow v_1 \quad e_2 \Downarrow v_2 \quad v = v_1 \land v_2}{e_1\, \texttt{and}\, e_2 \Downarrow v}
\end{aligned}
$$
The rule fully evaluates the left and right sub-expressions and then uses the logical and operator (\\(\land\\)) to compute the output. This second set of rules capture the *short-circuiting behavior*:
$$
\begin{aligned}
\frac{e_1 \Downarrow \texttt{False}}{e_1\, \texttt{and}\, e_2 \Downarrow \texttt{False}}
\\\\
\frac{e_1 \Downarrow \texttt{True} \quad e_2 \Downarrow \texttt{False}}{e_1\, \texttt{and}\, e_2 \Downarrow \texttt{False}}
\\\\
\frac{e_1 \Downarrow \texttt{True} \quad e_2 \Downarrow \texttt{True}}{e_1\, \texttt{and}\, e_2 \Downarrow \texttt{True}}
\end{aligned}
$$
The first rule skips the reduction of \\(e_2\\) if \\(e_1\\) evaluate to `False`.

The correct way to encode this short-circuiting behavior is to use conditionals:
```ocaml
e1 && e2
(* Rewrite into *)
let x1 = e1 in
if x1 then e2 else False
```

So, the syntax-based definition of a transpiler is wrong if we care about maintain correctness. This second definition of "transpiler" uses the idea of semantics:

> **Transpilers are semantics translators.** Given two language \\(S\\) and \\(T\\) with semantics \\(\Downarrow_s\\) and \\(\Downarrow_t\\), a *Transpiler* is function \\(\mathcal{C}: S \rightarrow T\\) such that \\(\forall s \in S : s \Downarrow_s v \implies C(s) \Downarrow_t v\\).

The math says that "if a source program \\(s\\) evaluates to \\(v\\), then the compiled program \\(C(s)\\) should also evaluate to \\(v\\)." In other words, a transpiler preserves the meaning of a program. This is a nice precise definition!
*It also happens to be the exact definition of a compiler.*

This puts us in a bind: The syntax translation definition of "transpilers" is too weak because it excludes transformations that so-called "transpilers" definitely support, while semantic translation definition is too strong because it includes normal compilers!

At this point, the wise critic would say that my academic top hat has blinded me and the obvious distinction between a transpiler and a compiler is that a transpiler is *simple*! Compilers can perform all sorts of complicated tricks to transform programs but the transpiler is elegant simplicity itself.

Let's see what math has to say about simplicity.

# Bills but no Webbed Feet

Simplicity is subjective but the history of the word "transpiler" might shed some light on what people mean. To my knowledge, "transpiler" originates within the JavaScript community and refers to tools like [Babel][] which enabled people to use new language constructs that lacked browser support. For example, JavaScript might implement a new `for`-`in` construct which allows a loop to walk over the keys of an object. However, browsers would take some time to implement the feature natively. In this case, Babel could compile `for`-`in` into an existing `while`-loop with the right function calls allowing codebases to adopt it while maintaining compatibility. Such a rewrite seems simpler than the heavyweight responsibilities of a compiler.

So, how do we define "simplicity"? Some readers might call the `for`-`in` construct *syntactic sugar*: it makes it easier to write certain kinds of programs but does not fundamentally change the *expressivity* of the language. An important property of syntactic sugar is that rewriting it is compositional: I can rewrite any `for`-`in` loop into the equivalent `while` loop without worrying about the surrounding context. This idea of "context independence" is precisely what the paper [On the Expressive Power of Programming Languages][expr-power] uses to formally distinguish between simple and complex rewrites:

> **Syntactic sugar.** A feature \\(\mathcal{F}\\) is *syntactic sugar* for language \\(\mathcal{L}\\) if there is a rewrite rule \\(\phi\\) such that \\(\phi\\) distributes over all composition operators of \\(\mathcal{L}\\). In other words, if \\(A(L_0, L_1, ...) \in \mathcal{L}\\), then \\(\phi(A(L_0, L_1, ..)) = A(\phi(L_0), \phi(L_1))\\). Such a \\(\phi\\) is called a *local transformation*.

Deep breath. The math looks like scary mumbo-jumbo but it describes what it means for a rewrite rule to be simple. In short, if the rewrite rule *distributes over* language constructs like loops (`while`), conditionals (`if`-`else`), and sequential composition (`;`), it is simple syntactic sugar. Because the rewriting can distribute over constructs, it cannot, for example, look at one part of the program and use that information to transform a different part of the program. This rules out transformations that add *expressive features* to a language–exceptions in languages with simple control flow or mutability in pure functional languages–which require global and contextual transformation.

I find this idea to be pretty neat because it allows to talk about what features of a language add expressivity to it:

> **Expressive constructs.**  A feature \\(\mathcal{F}\\) adds expressivity to \\(\mathcal{L}\\) if there does not exist a local transformation to eliminate it.

With these definitions, here is my third definition of a transpiler that uses the idea of *expressivity* and *local transformations*:

> **Transpilers are local rewriters.** *Transpilers* convert programs in source program \\(S\\) to target \\(T\\) by only performing local transformations.

This definition of "transpiler" is the only one I have any sympathy for since it captures the loose notion of simplicity that people often throw around. However, this definition is still far from being good enough because real programming languages have complex semantics that generally do require some amount of global transformation. For example, correctly implementing the semantics of JavaScript's `this` within Python would give anyone a headache.

# Call It What You Want

To summarize, I have given three definitions of the word "transpiler" grounded in formal concepts:
- **Syntax Translator.**  Based on the formalism of *concrete syntax*. This probably most closely matches the intuition people have when they say "transpiler" but falls apart upon basic scrutiny.
- **Semantic Translator.**  Based on the formalism of *semantics*. This is the definition of a compiler. If you mean this, then just use the word "compiler".
- **Local Rewriter.**  Based on *expressivity of constructs*. This definition is a little more nuanced and the one I am willing to cede some ground on. However, tools that are called "transpilers" often need to perform more than just local rewrites if they care about being correct.

I hope that, even if you disagree with my definitions, the formal tools shed some light on how my research community thinks about these problems. At the end of the day, I do not care what you call these tools; maybe there is some intuitive idea that is clear to your audience when you say "transpiler". However, in a world full of vagaries and vibes, I look for precise thought and I find the definitions of the word "transpiler" lacking.

[expr-power]: https://www2.ccs.neu.edu/racket/pubs/scp91-felleisen.pdf
[babel]: https://babeljs.io/
